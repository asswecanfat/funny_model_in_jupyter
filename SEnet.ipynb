{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import expand_dims\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import Layer, Dense, GlobalAveragePooling2D, ReLU, Activation, Conv2D, BatchNormalization, MaxPooling2D, Flatten, multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class SE_Module(Layer):\n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super(SE_Module, self).__init__()\n",
    "        self._avg_pool = GlobalAveragePooling2D()\n",
    "        self._d1 = Dense(1 * 1 * (channels / reduction))\n",
    "        self._r = ReLU()\n",
    "        self._d2 = Dense(1 * 1 * channels)\n",
    "        self._act = Activation(\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        output = self._avg_pool(inputs)\n",
    "        output = self._d1(output)\n",
    "        output = self._r(output)\n",
    "        output = self._d2(output)\n",
    "        output = self._act(output)\n",
    "        output = expand_dims(input=output, axis=1)\n",
    "        output = expand_dims(input=output, axis=1)\n",
    "        return multiply([inputs, output])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class SENet(Model):\n",
    "    def get_config(self):\n",
    "        super(SENet, self).get_config()\n",
    "\n",
    "    def __init__(self, classes: int):\n",
    "        super(SENet, self).__init__()\n",
    "        self._con1 = Conv2D(filters=32, kernel_size=(3, 3), strides=2, padding=\"same\")\n",
    "        self._a1 = ReLU()\n",
    "        self._b1 = BatchNormalization()\n",
    "        self._m1 = MaxPooling2D(pool_size=(3, 3), strides=1, padding=\"same\")\n",
    "\n",
    "        self._con2 = Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding=\"same\")\n",
    "        self._a2 = ReLU()\n",
    "        self._b2 = BatchNormalization()\n",
    "        self._m2 = MaxPooling2D(pool_size=(3, 3), strides=1, padding=\"same\")\n",
    "        # self._se2 = SE_Module(channels=64)\n",
    "\n",
    "        # self._con3 = Conv2D(filters=128, kernel_size=(3, 3), strides=2, padding=\"same\")\n",
    "        # self._a3 = ReLU()\n",
    "        # self._b3 = BatchNormalization()\n",
    "        # self._m3 = MaxPooling2D(pool_size=(3, 3), strides=1, padding=\"same\")\n",
    "        self._se2 = SE_Module(channels=64)\n",
    "        self._avgpool = GlobalAveragePooling2D()\n",
    "        self._fc = Dense(classes)\n",
    "        self._a4 = Activation(\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        output = self._con1(inputs)\n",
    "        output = self._a1(output)\n",
    "        output = self._b1(output)\n",
    "        output = self._m1(output)\n",
    "        # output = self._se1(output)\n",
    "\n",
    "        output = self._con2(output)\n",
    "        output = self._a2(output)\n",
    "        output = self._b2(output)\n",
    "        output = self._m2(output)\n",
    "        output = self._se2(output)\n",
    "\n",
    "        output = self._avgpool(output)\n",
    "        output = self._fc(output)\n",
    "        output = self._a4(output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"se_net_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           multiple                  896       \n",
      "_________________________________________________________________\n",
      "re_lu_12 (ReLU)              multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc multiple                  128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           multiple                  18496     \n",
      "_________________________________________________________________\n",
      "re_lu_13 (ReLU)              multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc multiple                  256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling multiple                  0         \n",
      "_________________________________________________________________\n",
      "se__module_2 (SE_Module)     multiple                  580       \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  130       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    multiple                  0         \n",
      "=================================================================\n",
      "Total params: 20,486\n",
      "Trainable params: 20,294\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "senet = SENet(classes=2)\n",
    "senet.build(input_shape=(None, 64, 64, 3))\n",
    "senet.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6404 images belonging to 2 classes.\n",
      "Found 1601 images belonging to 2 classes.\n",
      "Found 2023 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "idg = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "train_img_gen = idg.flow_from_directory(\"data/training_set\", target_size=(64, 64),\n",
    "                                        class_mode='categorical', subset='training')\n",
    "valid_img_gen = idg.flow_from_directory(\"data/training_set\", target_size=(64, 64),\n",
    "                                        class_mode='categorical', subset='validation')\n",
    "test_idg = ImageDataGenerator(rescale=1./255)\n",
    "test_img_gen = test_idg.flow_from_directory(\"data/test_set\", target_size=(64, 64),\n",
    "                                       class_mode='categorical')\n",
    "\n",
    "assert train_img_gen.class_indices == test_img_gen.class_indices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "201/201 [==============================] - 11s 50ms/step - loss: 0.6154 - acc: 0.6651 - val_loss: 1.0446 - val_acc: 0.5347\n",
      "Epoch 2/100\n",
      "201/201 [==============================] - 10s 49ms/step - loss: 0.6045 - acc: 0.6694 - val_loss: 1.2373 - val_acc: 0.5016\n",
      "Epoch 3/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.5990 - acc: 0.6730 - val_loss: 1.0482 - val_acc: 0.5347\n",
      "Epoch 4/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.5878 - acc: 0.6855 - val_loss: 0.6389 - val_acc: 0.6184\n",
      "Epoch 5/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.5758 - acc: 0.6936 - val_loss: 0.6347 - val_acc: 0.6615\n",
      "Epoch 6/100\n",
      "201/201 [==============================] - 11s 53ms/step - loss: 0.5710 - acc: 0.7024 - val_loss: 0.8126 - val_acc: 0.5472\n",
      "Epoch 7/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.5618 - acc: 0.7092 - val_loss: 0.8533 - val_acc: 0.5678\n",
      "Epoch 8/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.5499 - acc: 0.7191 - val_loss: 1.1327 - val_acc: 0.5303\n",
      "Epoch 9/100\n",
      "201/201 [==============================] - 11s 56ms/step - loss: 0.5382 - acc: 0.7303 - val_loss: 0.6772 - val_acc: 0.6508\n",
      "Epoch 10/100\n",
      "201/201 [==============================] - 16s 78ms/step - loss: 0.5371 - acc: 0.7306 - val_loss: 2.8485 - val_acc: 0.5215\n",
      "Epoch 11/100\n",
      "201/201 [==============================] - 11s 53ms/step - loss: 0.5344 - acc: 0.7339 - val_loss: 0.6744 - val_acc: 0.6390\n",
      "Epoch 12/100\n",
      "201/201 [==============================] - 10s 49ms/step - loss: 0.5158 - acc: 0.7409 - val_loss: 0.6191 - val_acc: 0.6571\n",
      "Epoch 13/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.5229 - acc: 0.7475 - val_loss: 0.6437 - val_acc: 0.6496\n",
      "Epoch 14/100\n",
      "201/201 [==============================] - 10s 51ms/step - loss: 0.5078 - acc: 0.7531 - val_loss: 1.1489 - val_acc: 0.5478\n",
      "Epoch 15/100\n",
      "201/201 [==============================] - 11s 54ms/step - loss: 0.4954 - acc: 0.7623 - val_loss: 0.5560 - val_acc: 0.7252\n",
      "Epoch 16/100\n",
      "201/201 [==============================] - 11s 53ms/step - loss: 0.4916 - acc: 0.7636 - val_loss: 0.6309 - val_acc: 0.6652\n",
      "Epoch 17/100\n",
      "201/201 [==============================] - 10s 52ms/step - loss: 0.4903 - acc: 0.7625 - val_loss: 0.5898 - val_acc: 0.7014\n",
      "Epoch 18/100\n",
      "201/201 [==============================] - 10s 51ms/step - loss: 0.4780 - acc: 0.7686 - val_loss: 0.5344 - val_acc: 0.7283\n",
      "Epoch 19/100\n",
      "201/201 [==============================] - 10s 51ms/step - loss: 0.4662 - acc: 0.7804 - val_loss: 0.6800 - val_acc: 0.6746\n",
      "Epoch 20/100\n",
      "201/201 [==============================] - 11s 52ms/step - loss: 0.4654 - acc: 0.7786 - val_loss: 0.9039 - val_acc: 0.5890\n",
      "Epoch 21/100\n",
      "201/201 [==============================] - 10s 52ms/step - loss: 0.4590 - acc: 0.7864 - val_loss: 0.8473 - val_acc: 0.6146\n",
      "Epoch 22/100\n",
      "201/201 [==============================] - 10s 52ms/step - loss: 0.4594 - acc: 0.7858 - val_loss: 0.5709 - val_acc: 0.7208\n",
      "Epoch 23/100\n",
      "201/201 [==============================] - 11s 54ms/step - loss: 0.4511 - acc: 0.7947 - val_loss: 2.0114 - val_acc: 0.5365\n",
      "Epoch 24/100\n",
      "201/201 [==============================] - 10s 51ms/step - loss: 0.4435 - acc: 0.7990 - val_loss: 0.6544 - val_acc: 0.6721\n",
      "Epoch 25/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.4422 - acc: 0.7986 - val_loss: 0.5468 - val_acc: 0.7127\n",
      "Epoch 26/100\n",
      "201/201 [==============================] - 10s 49ms/step - loss: 0.4317 - acc: 0.8004 - val_loss: 0.5858 - val_acc: 0.7433\n",
      "Epoch 27/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.4235 - acc: 0.8075 - val_loss: 0.5913 - val_acc: 0.7071\n",
      "Epoch 28/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.4176 - acc: 0.8140 - val_loss: 0.6337 - val_acc: 0.7046\n",
      "Epoch 29/100\n",
      "201/201 [==============================] - 11s 52ms/step - loss: 0.4224 - acc: 0.8073 - val_loss: 0.6147 - val_acc: 0.7289\n",
      "Epoch 30/100\n",
      "201/201 [==============================] - 10s 52ms/step - loss: 0.4146 - acc: 0.8131 - val_loss: 0.5999 - val_acc: 0.7352\n",
      "Epoch 31/100\n",
      "201/201 [==============================] - 10s 51ms/step - loss: 0.4140 - acc: 0.8159 - val_loss: 0.5797 - val_acc: 0.7295\n",
      "Epoch 32/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.4102 - acc: 0.8097 - val_loss: 0.5928 - val_acc: 0.7077\n",
      "Epoch 33/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.4002 - acc: 0.8228 - val_loss: 0.5190 - val_acc: 0.7552\n",
      "Epoch 34/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.4002 - acc: 0.8243 - val_loss: 0.5862 - val_acc: 0.7458\n",
      "Epoch 35/100\n",
      "201/201 [==============================] - 10s 51ms/step - loss: 0.3976 - acc: 0.8200 - val_loss: 0.6861 - val_acc: 0.6996\n",
      "Epoch 36/100\n",
      "201/201 [==============================] - 10s 52ms/step - loss: 0.3998 - acc: 0.8178 - val_loss: 0.7646 - val_acc: 0.6733\n",
      "Epoch 37/100\n",
      "201/201 [==============================] - 10s 52ms/step - loss: 0.3823 - acc: 0.8300 - val_loss: 0.7301 - val_acc: 0.6827\n",
      "Epoch 38/100\n",
      "201/201 [==============================] - 11s 53ms/step - loss: 0.3727 - acc: 0.8328 - val_loss: 0.5219 - val_acc: 0.7689\n",
      "Epoch 39/100\n",
      "201/201 [==============================] - 10s 52ms/step - loss: 0.3807 - acc: 0.8285 - val_loss: 0.9300 - val_acc: 0.6227\n",
      "Epoch 40/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.3803 - acc: 0.8309 - val_loss: 0.5222 - val_acc: 0.7514\n",
      "Epoch 41/100\n",
      "201/201 [==============================] - 10s 52ms/step - loss: 0.3761 - acc: 0.8293 - val_loss: 0.6649 - val_acc: 0.7033\n",
      "Epoch 42/100\n",
      "201/201 [==============================] - 10s 52ms/step - loss: 0.3649 - acc: 0.8353 - val_loss: 1.5933 - val_acc: 0.5353\n",
      "Epoch 43/100\n",
      "201/201 [==============================] - 10s 51ms/step - loss: 0.3666 - acc: 0.8382 - val_loss: 0.7293 - val_acc: 0.6983\n",
      "Epoch 44/100\n",
      "201/201 [==============================] - 11s 53ms/step - loss: 0.3466 - acc: 0.8471 - val_loss: 0.5169 - val_acc: 0.7758\n",
      "Epoch 45/100\n",
      "201/201 [==============================] - 10s 51ms/step - loss: 0.3577 - acc: 0.8428 - val_loss: 0.5376 - val_acc: 0.7689\n",
      "Epoch 46/100\n",
      "201/201 [==============================] - 11s 54ms/step - loss: 0.3502 - acc: 0.8409 - val_loss: 0.6344 - val_acc: 0.7171\n",
      "Epoch 47/100\n",
      "201/201 [==============================] - 10s 52ms/step - loss: 0.3466 - acc: 0.8428 - val_loss: 0.5872 - val_acc: 0.7495\n",
      "Epoch 48/100\n",
      "201/201 [==============================] - 10s 52ms/step - loss: 0.3342 - acc: 0.8520 - val_loss: 0.7626 - val_acc: 0.6977\n",
      "Epoch 49/100\n",
      "201/201 [==============================] - 10s 52ms/step - loss: 0.3385 - acc: 0.8527 - val_loss: 0.4967 - val_acc: 0.7820\n",
      "Epoch 50/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.3338 - acc: 0.8540 - val_loss: 0.5629 - val_acc: 0.7377\n",
      "Epoch 51/100\n",
      "201/201 [==============================] - 10s 51ms/step - loss: 0.3353 - acc: 0.8509 - val_loss: 0.7485 - val_acc: 0.6821\n",
      "Epoch 52/100\n",
      "201/201 [==============================] - 11s 52ms/step - loss: 0.3319 - acc: 0.8598 - val_loss: 1.1627 - val_acc: 0.5734\n",
      "Epoch 53/100\n",
      "201/201 [==============================] - 11s 54ms/step - loss: 0.3152 - acc: 0.8648 - val_loss: 0.5272 - val_acc: 0.7664\n",
      "Epoch 54/100\n",
      "201/201 [==============================] - 11s 53ms/step - loss: 0.3097 - acc: 0.8646 - val_loss: 0.7828 - val_acc: 0.7158\n",
      "Epoch 55/100\n",
      "201/201 [==============================] - 11s 53ms/step - loss: 0.3123 - acc: 0.8641 - val_loss: 0.8759 - val_acc: 0.6933\n",
      "Epoch 56/100\n",
      "201/201 [==============================] - 11s 53ms/step - loss: 0.2979 - acc: 0.8745 - val_loss: 0.5518 - val_acc: 0.7445\n",
      "Epoch 57/100\n",
      "201/201 [==============================] - 12s 58ms/step - loss: 0.3048 - acc: 0.8695 - val_loss: 0.5813 - val_acc: 0.7439\n",
      "Epoch 58/100\n",
      "201/201 [==============================] - 10s 51ms/step - loss: 0.2987 - acc: 0.8721 - val_loss: 0.7557 - val_acc: 0.6983\n",
      "Epoch 59/100\n",
      "201/201 [==============================] - 11s 55ms/step - loss: 0.3041 - acc: 0.8691 - val_loss: 1.3242 - val_acc: 0.6252\n",
      "Epoch 60/100\n",
      "201/201 [==============================] - 10s 52ms/step - loss: 0.2875 - acc: 0.8757 - val_loss: 0.5616 - val_acc: 0.7708\n",
      "Epoch 61/100\n",
      "201/201 [==============================] - 11s 56ms/step - loss: 0.2902 - acc: 0.8795 - val_loss: 0.7775 - val_acc: 0.7414\n",
      "Epoch 62/100\n",
      "201/201 [==============================] - 11s 56ms/step - loss: 0.2939 - acc: 0.8738 - val_loss: 0.5610 - val_acc: 0.7676\n",
      "Epoch 63/100\n",
      "201/201 [==============================] - 12s 59ms/step - loss: 0.2664 - acc: 0.8901 - val_loss: 0.5276 - val_acc: 0.7770\n",
      "Epoch 64/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.3146 - acc: 0.8638 - val_loss: 0.5995 - val_acc: 0.7601\n",
      "Epoch 65/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.2892 - acc: 0.8763 - val_loss: 0.5107 - val_acc: 0.7814\n",
      "Epoch 66/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.2703 - acc: 0.8854 - val_loss: 0.7576 - val_acc: 0.6958\n",
      "Epoch 67/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.2821 - acc: 0.8787 - val_loss: 0.7660 - val_acc: 0.7152\n",
      "Epoch 68/100\n",
      "201/201 [==============================] - 11s 54ms/step - loss: 0.2660 - acc: 0.8868 - val_loss: 0.5652 - val_acc: 0.7658\n",
      "Epoch 69/100\n",
      "201/201 [==============================] - 11s 53ms/step - loss: 0.2539 - acc: 0.8937 - val_loss: 0.5614 - val_acc: 0.7739\n",
      "Epoch 70/100\n",
      "201/201 [==============================] - 10s 52ms/step - loss: 0.2632 - acc: 0.8888 - val_loss: 0.8979 - val_acc: 0.7164\n",
      "Epoch 71/100\n",
      "201/201 [==============================] - 10s 48ms/step - loss: 0.2686 - acc: 0.8859 - val_loss: 0.7463 - val_acc: 0.7258\n",
      "Epoch 72/100\n",
      "201/201 [==============================] - 10s 49ms/step - loss: 0.2482 - acc: 0.8958 - val_loss: 0.7510 - val_acc: 0.7189\n",
      "Epoch 73/100\n",
      "201/201 [==============================] - 10s 48ms/step - loss: 0.2424 - acc: 0.8998 - val_loss: 0.5789 - val_acc: 0.7539\n",
      "Epoch 74/100\n",
      "201/201 [==============================] - 10s 48ms/step - loss: 0.2398 - acc: 0.8998 - val_loss: 0.8643 - val_acc: 0.7089\n",
      "Epoch 75/100\n",
      "201/201 [==============================] - 10s 48ms/step - loss: 0.2364 - acc: 0.9005 - val_loss: 0.5857 - val_acc: 0.7801\n",
      "Epoch 76/100\n",
      "201/201 [==============================] - 10s 47ms/step - loss: 0.2237 - acc: 0.9083 - val_loss: 0.5493 - val_acc: 0.7826\n",
      "Epoch 77/100\n",
      "201/201 [==============================] - 9s 47ms/step - loss: 0.2407 - acc: 0.8998 - val_loss: 0.6509 - val_acc: 0.7633\n",
      "Epoch 78/100\n",
      "201/201 [==============================] - 10s 48ms/step - loss: 0.2250 - acc: 0.9076 - val_loss: 0.5414 - val_acc: 0.8007\n",
      "Epoch 79/100\n",
      "201/201 [==============================] - 10s 48ms/step - loss: 0.2434 - acc: 0.8966 - val_loss: 0.6552 - val_acc: 0.7645\n",
      "Epoch 80/100\n",
      "201/201 [==============================] - 10s 47ms/step - loss: 0.2219 - acc: 0.9072 - val_loss: 0.5983 - val_acc: 0.7851\n",
      "Epoch 81/100\n",
      "201/201 [==============================] - 10s 47ms/step - loss: 0.2105 - acc: 0.9152 - val_loss: 0.6118 - val_acc: 0.7933\n",
      "Epoch 82/100\n",
      "201/201 [==============================] - 10s 48ms/step - loss: 0.2254 - acc: 0.9041 - val_loss: 0.8333 - val_acc: 0.7689\n",
      "Epoch 83/100\n",
      "201/201 [==============================] - 10s 49ms/step - loss: 0.2043 - acc: 0.9152 - val_loss: 0.8077 - val_acc: 0.7327\n",
      "Epoch 84/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.2259 - acc: 0.9040 - val_loss: 0.6202 - val_acc: 0.7527\n",
      "Epoch 85/100\n",
      "201/201 [==============================] - 10s 49ms/step - loss: 0.2266 - acc: 0.9012 - val_loss: 0.6518 - val_acc: 0.7458\n",
      "Epoch 86/100\n",
      "201/201 [==============================] - 10s 49ms/step - loss: 0.2177 - acc: 0.9062 - val_loss: 0.5548 - val_acc: 0.7864\n",
      "Epoch 87/100\n",
      "201/201 [==============================] - 10s 48ms/step - loss: 0.1991 - acc: 0.9166 - val_loss: 0.8956 - val_acc: 0.7208\n",
      "Epoch 88/100\n",
      "201/201 [==============================] - 10s 49ms/step - loss: 0.1966 - acc: 0.9193 - val_loss: 0.9410 - val_acc: 0.7408\n",
      "Epoch 89/100\n",
      "201/201 [==============================] - 10s 48ms/step - loss: 0.2008 - acc: 0.9177 - val_loss: 0.6431 - val_acc: 0.7633\n",
      "Epoch 90/100\n",
      "201/201 [==============================] - 10s 48ms/step - loss: 0.2052 - acc: 0.9169 - val_loss: 0.6712 - val_acc: 0.7689\n",
      "Epoch 91/100\n",
      "201/201 [==============================] - 10s 48ms/step - loss: 0.1980 - acc: 0.9158 - val_loss: 0.6244 - val_acc: 0.7858\n",
      "Epoch 92/100\n",
      "201/201 [==============================] - 10s 49ms/step - loss: 0.2065 - acc: 0.9138 - val_loss: 0.6663 - val_acc: 0.7414\n",
      "Epoch 93/100\n",
      "201/201 [==============================] - 10s 48ms/step - loss: 0.1827 - acc: 0.9296 - val_loss: 0.6687 - val_acc: 0.7745\n",
      "Epoch 94/100\n",
      "201/201 [==============================] - 10s 48ms/step - loss: 0.1832 - acc: 0.9250 - val_loss: 0.6051 - val_acc: 0.7826\n",
      "Epoch 95/100\n",
      "201/201 [==============================] - 10s 48ms/step - loss: 0.1807 - acc: 0.9219 - val_loss: 0.7586 - val_acc: 0.7464\n",
      "Epoch 96/100\n",
      "201/201 [==============================] - 10s 48ms/step - loss: 0.1749 - acc: 0.9280 - val_loss: 0.7017 - val_acc: 0.7577\n",
      "Epoch 97/100\n",
      "201/201 [==============================] - 10s 48ms/step - loss: 0.1721 - acc: 0.9316 - val_loss: 0.5923 - val_acc: 0.7639\n",
      "Epoch 98/100\n",
      "201/201 [==============================] - 10s 50ms/step - loss: 0.2193 - acc: 0.9069 - val_loss: 0.9633 - val_acc: 0.6958\n",
      "Epoch 99/100\n",
      "201/201 [==============================] - 10s 49ms/step - loss: 0.1865 - acc: 0.9227 - val_loss: 0.7728 - val_acc: 0.7651\n",
      "Epoch 100/100\n",
      "201/201 [==============================] - 10s 49ms/step - loss: 0.1990 - acc: 0.9194 - val_loss: 0.6736 - val_acc: 0.7745\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "senet.compile(optimizer=sgd, loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "history = senet.fit(train_img_gen, validation_data=valid_img_gen, epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 36ms/step - loss: 0.6922 - acc: 0.7672\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[5.71659170e-02, 9.45141792e-01],\n       [9.56549719e-02, 8.99565041e-01],\n       [9.98662353e-01, 1.38015754e-03],\n       [6.58368299e-05, 9.99934077e-01],\n       [7.88451254e-01, 2.25163341e-01],\n       [9.42080002e-03, 9.90814626e-01],\n       [9.76712525e-01, 2.54333168e-02],\n       [9.50281918e-01, 5.51555119e-02],\n       [8.21961323e-04, 9.99243140e-01],\n       [1.65730596e-01, 8.53235424e-01],\n       [9.07347083e-01, 1.09939903e-01],\n       [6.75304700e-03, 9.94063914e-01],\n       [9.66065098e-04, 9.99089956e-01],\n       [3.74713033e-01, 6.39181912e-01],\n       [8.54200006e-01, 1.53269857e-01],\n       [1.45217905e-06, 9.99998689e-01],\n       [9.70629394e-01, 3.20013352e-02],\n       [2.94716447e-03, 9.96960700e-01],\n       [2.40428358e-01, 7.92013228e-01],\n       [2.47849464e-01, 7.43531287e-01],\n       [6.99926138e-01, 3.14027190e-01],\n       [1.85606295e-05, 9.99980211e-01],\n       [6.40053770e-08, 9.99999881e-01],\n       [4.12293058e-03, 9.95705426e-01],\n       [2.61089981e-01, 7.57140100e-01],\n       [9.98796821e-01, 1.29553850e-03],\n       [2.35690057e-01, 7.58204103e-01],\n       [7.26244867e-01, 2.81565815e-01],\n       [1.13507761e-02, 9.88600433e-01],\n       [2.91522950e-01, 7.41688371e-01],\n       [1.08409324e-04, 9.99879360e-01],\n       [7.97650814e-01, 2.10759893e-01]], dtype=float32)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senet.evaluate(test_img_gen)\n",
    "senet.predict(next(test_img_gen)[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}