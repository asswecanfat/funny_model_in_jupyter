{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, ReLU, GlobalAveragePooling2D, Input, Flatten, Conv1D, Layer\n",
    "from tensorflow import random, Variable, multiply, transpose, Tensor, reshape\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.backend import sigmoid, softmax\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class Weight(Layer):\n",
    "    \"\"\"\n",
    "    sECAnet的select权重设计\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Weight, self).__init__()\n",
    "        self.weight = Variable(random.uniform([1], 0, 1), trainable=True, name=\"se-W\")\n",
    "\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        return inputs[0] * self.weight + (1. - self.weight) * inputs[1]\n",
    "\n",
    "\n",
    "def channel_shuffle(input_tensor: Tensor, group_num: int = 2) -> Tensor:\n",
    "    \"\"\"\n",
    "    参考：https://blog.csdn.net/baidu_23388287/article/details/94456951\n",
    "         https://blog.csdn.net/qq_36758914/article/details/106967780\n",
    "\n",
    "    :param input_tensor: 输入特征图数据\n",
    "    :param group_num: 分组\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    batch_size, h, w, channel = input_tensor.get_shape()\n",
    "    # 注意下面的//一定要整除\n",
    "    input_reshaped = reshape(input_tensor, [-1, h, w, group_num, channel // group_num])\n",
    "    input_transpose = transpose(input_reshaped, [0, 1, 2, 4, 3])\n",
    "    return reshape(input_transpose, [-1, h, w, channel])\n",
    "\n",
    "\n",
    "def sECAnet(input_shape: tuple[int, int, int], classes: int) -> Model:\n",
    "    \"\"\"\n",
    "    sECAnet构建\n",
    "\n",
    "    :param input_shape: 输入形状\n",
    "    :param classes: 分类数\n",
    "    :return: sECAnet\n",
    "    \"\"\"\n",
    "    x = Input(shape=input_shape)\n",
    "    y = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "    y = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\")(y)\n",
    "    y_ = Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\")(y)\n",
    "    # 注意力1\n",
    "    a1 = GlobalAveragePooling2D(keepdims=True)(y_)\n",
    "    # 打乱\n",
    "    a1 = channel_shuffle(a1)\n",
    "    a1 = Conv1D(filters=a1.shape[-1], kernel_size=3, padding=\"same\")(a1)\n",
    "    a1 = sigmoid(a1)\n",
    "    # 注意力2\n",
    "    a2 = GlobalAveragePooling2D(keepdims=True)(y_)\n",
    "    a2 = Conv1D(filters=a2.shape[-1], kernel_size=3, padding=\"same\")(a2)\n",
    "    a2 = sigmoid(a2)\n",
    "    # select\n",
    "\n",
    "    attention_v = Weight()((a1, a2))\n",
    "    # 相乘\n",
    "    y = multiply(attention_v, y_)\n",
    "\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(512)(y)\n",
    "    y = ReLU()(y)\n",
    "    y = Dense(classes)(y)\n",
    "    y = softmax(y)\n",
    "    return Model(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 28, 28, 64)   640         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 28, 28, 128)  73856       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 28, 28, 256)  295168      conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 1, 1, 256)    0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape_4 (TFOpLambda)       (None, 1, 1, 2, 128) 0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.transpose_2 (TFOpL (None, 1, 1, 128, 2) 0           tf.reshape_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape_5 (TFOpLambda)       (None, 1, 1, 256)    0           tf.compat.v1.transpose_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 1, 1, 256)    0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1, 1, 256)    196864      tf.reshape_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1, 1, 256)    196864      global_average_pooling2d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sigmoid_4 (TFOpLambda)  (None, 1, 1, 256)    0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sigmoid_5 (TFOpLambda)  (None, 1, 1, 256)    0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "weight_2 (Weight)               (None, 1, 1, 256)    1           tf.math.sigmoid_4[0][0]          \n",
      "                                                                 tf.math.sigmoid_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_2 (TFOpLambda) (None, 28, 28, 256)  0           weight_2[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 200704)       0           tf.math.multiply_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          102760960   flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 512)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           5130        re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.nn.softmax_2 (TFOpLambda)    (None, 10)           0           dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 103,529,483\n",
      "Trainable params: 103,529,483\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "snet = sECAnet((28, 28, 1), 10)\n",
    "snet.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "\n",
    "def data_load():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train / 255.\n",
    "    x_test = x_test / 255.\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'se-W:0' shape=() dtype=float32, numpy=0.9>\n",
      "Epoch 1/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 1.2189 - acc: 0.7643 - val_loss: 0.3891 - val_acc: 0.8926\n",
      "Epoch 2/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.3552 - acc: 0.8986 - val_loss: 0.3036 - val_acc: 0.9124\n",
      "Epoch 3/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.2913 - acc: 0.9158 - val_loss: 0.2565 - val_acc: 0.9260\n",
      "Epoch 4/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.2512 - acc: 0.9275 - val_loss: 0.2327 - val_acc: 0.9309\n",
      "Epoch 5/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.2207 - acc: 0.9356 - val_loss: 0.1930 - val_acc: 0.9431\n",
      "Epoch 6/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.1957 - acc: 0.9431 - val_loss: 0.1773 - val_acc: 0.9479\n",
      "Epoch 7/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.1754 - acc: 0.9483 - val_loss: 0.1641 - val_acc: 0.9503\n",
      "Epoch 8/50\n",
      "938/938 [==============================] - 68s 73ms/step - loss: 0.1585 - acc: 0.9528 - val_loss: 0.1443 - val_acc: 0.9580\n",
      "Epoch 9/50\n",
      "938/938 [==============================] - 68s 73ms/step - loss: 0.1449 - acc: 0.9578 - val_loss: 0.1323 - val_acc: 0.9618\n",
      "Epoch 10/50\n",
      "938/938 [==============================] - 68s 73ms/step - loss: 0.1333 - acc: 0.9606 - val_loss: 0.1195 - val_acc: 0.9664\n",
      "Epoch 11/50\n",
      "938/938 [==============================] - 68s 73ms/step - loss: 0.1223 - acc: 0.9638 - val_loss: 0.1140 - val_acc: 0.9668\n",
      "Epoch 12/50\n",
      "938/938 [==============================] - 68s 73ms/step - loss: 0.1136 - acc: 0.9664 - val_loss: 0.1138 - val_acc: 0.9644\n",
      "Epoch 13/50\n",
      "938/938 [==============================] - 68s 73ms/step - loss: 0.1060 - acc: 0.9682 - val_loss: 0.1000 - val_acc: 0.9701\n",
      "Epoch 14/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0994 - acc: 0.9705 - val_loss: 0.1040 - val_acc: 0.9678\n",
      "Epoch 15/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0933 - acc: 0.9723 - val_loss: 0.0911 - val_acc: 0.9712\n",
      "Epoch 16/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0876 - acc: 0.9735 - val_loss: 0.0930 - val_acc: 0.9709\n",
      "Epoch 17/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0830 - acc: 0.9752 - val_loss: 0.0905 - val_acc: 0.9714\n",
      "Epoch 18/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0786 - acc: 0.9764 - val_loss: 0.0839 - val_acc: 0.9731\n",
      "Epoch 19/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0751 - acc: 0.9775 - val_loss: 0.1005 - val_acc: 0.9685\n",
      "Epoch 20/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0719 - acc: 0.9775 - val_loss: 0.0802 - val_acc: 0.9753\n",
      "Epoch 21/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0686 - acc: 0.9791 - val_loss: 0.0801 - val_acc: 0.9745\n",
      "Epoch 22/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0654 - acc: 0.9803 - val_loss: 0.0962 - val_acc: 0.9699\n",
      "Epoch 23/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0624 - acc: 0.9808 - val_loss: 0.0748 - val_acc: 0.9762\n",
      "Epoch 24/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0600 - acc: 0.9811 - val_loss: 0.0828 - val_acc: 0.9737\n",
      "Epoch 25/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0571 - acc: 0.9824 - val_loss: 0.0888 - val_acc: 0.9718\n",
      "Epoch 26/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0553 - acc: 0.9832 - val_loss: 0.0708 - val_acc: 0.9770\n",
      "Epoch 27/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0527 - acc: 0.9840 - val_loss: 0.0765 - val_acc: 0.9759\n",
      "Epoch 28/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0510 - acc: 0.9844 - val_loss: 0.0827 - val_acc: 0.9719\n",
      "Epoch 29/50\n",
      "938/938 [==============================] - 67s 72ms/step - loss: 0.0496 - acc: 0.9847 - val_loss: 0.0791 - val_acc: 0.9758\n",
      "Epoch 30/50\n",
      "938/938 [==============================] - 67s 72ms/step - loss: 0.0467 - acc: 0.9853 - val_loss: 0.0746 - val_acc: 0.9760\n",
      "Epoch 31/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0455 - acc: 0.9859 - val_loss: 0.0682 - val_acc: 0.9780\n",
      "Epoch 32/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0433 - acc: 0.9867 - val_loss: 0.0710 - val_acc: 0.9784\n",
      "Epoch 33/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0421 - acc: 0.9867 - val_loss: 0.0710 - val_acc: 0.9782\n",
      "Epoch 34/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0409 - acc: 0.9874 - val_loss: 0.0658 - val_acc: 0.9795\n",
      "Epoch 35/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0390 - acc: 0.9878 - val_loss: 0.0664 - val_acc: 0.9808\n",
      "Epoch 36/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0378 - acc: 0.9881 - val_loss: 0.0696 - val_acc: 0.9800\n",
      "Epoch 37/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0362 - acc: 0.9887 - val_loss: 0.0632 - val_acc: 0.9815\n",
      "Epoch 38/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0353 - acc: 0.9888 - val_loss: 0.0643 - val_acc: 0.9801\n",
      "Epoch 39/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0349 - acc: 0.9890 - val_loss: 0.0644 - val_acc: 0.9804\n",
      "Epoch 40/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0339 - acc: 0.9893 - val_loss: 0.0612 - val_acc: 0.9808\n",
      "Epoch 41/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0315 - acc: 0.9902 - val_loss: 0.0652 - val_acc: 0.9808\n",
      "Epoch 42/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0315 - acc: 0.9902 - val_loss: 0.0683 - val_acc: 0.9791\n",
      "Epoch 43/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0306 - acc: 0.9902 - val_loss: 0.0610 - val_acc: 0.9800\n",
      "Epoch 44/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0286 - acc: 0.9912 - val_loss: 0.0644 - val_acc: 0.9809\n",
      "Epoch 45/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0272 - acc: 0.9917 - val_loss: 0.0599 - val_acc: 0.9809\n",
      "Epoch 46/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0271 - acc: 0.9913 - val_loss: 0.0576 - val_acc: 0.9824\n",
      "Epoch 47/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0251 - acc: 0.9925 - val_loss: 0.0652 - val_acc: 0.9804\n",
      "Epoch 48/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0254 - acc: 0.9920 - val_loss: 0.0651 - val_acc: 0.9800\n",
      "Epoch 49/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0238 - acc: 0.9926 - val_loss: 0.0657 - val_acc: 0.9804\n",
      "Epoch 50/50\n",
      "938/938 [==============================] - 68s 72ms/step - loss: 0.0230 - acc: 0.9928 - val_loss: 0.0693 - val_acc: 0.9797\n",
      "<tf.Variable 'se-W:0' shape=() dtype=float32, numpy=0.95358884>\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(learning_rate=0.001)\n",
    "(x_train, y_train), (x_test, y_test) = data_load()\n",
    "snet.compile(optimizer=sgd, loss=SparseCategoricalCrossentropy(), metrics=['acc'])\n",
    "print(snet.trainable_variables[-5])\n",
    "snet.fit(x=x_train, y=y_train, validation_data=(x_test, y_test), batch_size=64, epochs=50, verbose=1)\n",
    "print(snet.trainable_variables[-5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 10, 10)            30        \n",
      "=================================================================\n",
      "Total params: 30\n",
      "Trainable params: 30\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "a = Sequential()\n",
    "a.add(Dense(10, input_shape=(10, 2)))\n",
    "a.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}